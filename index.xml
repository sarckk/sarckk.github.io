<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Bits&amp;Brains</title><link>https://sarckk.github.io/</link><description>Recent content in Home on Bits&amp;Brains</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 10 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://sarckk.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Grokking Transformers</title><link>https://sarckk.github.io/post/2023/04/10/grokking-transformers/</link><pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate><guid>https://sarckk.github.io/post/2023/04/10/grokking-transformers/</guid><description>Today&amp;rsquo;s Large Language Models (LLM) are based on Transformers, a deep learning model architecture for sequence-to-sequence transformations based on the attention mechanism. While it was originally proposed and used in Natural Language Processing (NLP) tasks like language translation, it turns out that a lot of things that we care about can be modelled in terms of sequences, making transformers a useful model in a wide variety of applications beyond NLP such as image processing and reinforcement learning .</description></item><item><title>Programming with Bing Chat</title><link>https://sarckk.github.io/post/2023/04/07/programming-with-bing-chat/</link><pubDate>Fri, 07 Apr 2023 00:00:00 +0000</pubDate><guid>https://sarckk.github.io/post/2023/04/07/programming-with-bing-chat/</guid><description>It&amp;rsquo;s been roughly 3 weeks since the release of GPT-4. In the world of AI that means I&amp;rsquo;m already late to the party, but with extra time this week I thought I&amp;rsquo;d finally start playing around with it to see what it can do. People have already started leveraging the power of GPT-4 to create impressive projects, like @ammaar who used GPT-4 and other AI tools like MidJourney to create a 3D game in Javascript from scratch and @mortenjust who made and published an iOS app by prompting GPT-4.</description></item><item><title>Things I learned recently</title><link>https://sarckk.github.io/post/2023/02/12/things-i-learned-recently/</link><pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate><guid>https://sarckk.github.io/post/2023/02/12/things-i-learned-recently/</guid><description>It&amp;rsquo;s recently been 6 months since I started my first full-time job as a software engineer after graduating from university last year. A lot has changed in my life, and I thought I&amp;rsquo;d write about the things I&amp;rsquo;ve learned about myself, career, and life in the past half a year.
Some of these are amalgamations of things I&amp;rsquo;ve read about before but have only recently begun to appreciate. It&amp;rsquo;s also things that I believe to be generally true at the time of writing this, but I suspect some of these will not apply to all situations (see learning #6) or will need to be updated with more nuance in the future.</description></item><item><title>Convolutional Neural Networks from Scratch</title><link>https://sarckk.github.io/post/2022/03/20/convolutional-neural-networks-from-scratch/</link><pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate><guid>https://sarckk.github.io/post/2022/03/20/convolutional-neural-networks-from-scratch/</guid><description>In this blog post we are going to take a look at how to implement a simple CNN model from scratch in Python, using mostly just numpy.
In practice, we can use high-level libraries such as Keras or PyTorch to abstract away the underlying details of CNN when writing code. However, we find that the exercise of writing one from scratch is very helpful in gaining a deeper understanding of CNNs and how these frameworks work under the hood.</description></item></channel></rss>