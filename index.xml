<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Bits&amp;Brains</title><link>https://sarckk.github.io/</link><description>Recent content in Home on Bits&amp;Brains</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 10 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://sarckk.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Grokking Transformers</title><link>https://sarckk.github.io/post/2023/04/10/grokking-transformers/</link><pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate><guid>https://sarckk.github.io/post/2023/04/10/grokking-transformers/</guid><description>&lt;p&gt;Today&amp;rsquo;s Large Language Models (LLM) are based on Transformers, a deep learning model architecture for sequence-to-sequence transformations based on the attention mechanism. While it was originally proposed and used in Natural Language Processing (NLP) tasks like language translation, it turns out that a lot of things that we care about can be modelled in terms of sequences, making transformers a useful model in a wide variety of applications beyond NLP such as &lt;a href="https://arxiv.org/abs/2103.14030" target="_blank" rel="noreferrer noopener"&gt;image processing&lt;/a&gt;
 and &lt;a href="https://arxiv.org/abs/2106.01345" target="_blank" rel="noreferrer noopener"&gt;reinforcement learning&lt;/a&gt;
. Given the overwhelming success of transformers in deep learning, I thought I should finally take some time to read and understand the paper &lt;a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer noopener"&gt;&amp;ldquo;Attention Is All You Need&amp;rdquo; (Vaswani et al., 2017)&lt;/a&gt;
 that first proposed the Transformer architecture.&lt;/p&gt;</description></item><item><title>Convolutional Neural Networks from Scratch</title><link>https://sarckk.github.io/post/2022/03/20/convolutional-neural-networks-from-scratch/</link><pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate><guid>https://sarckk.github.io/post/2022/03/20/convolutional-neural-networks-from-scratch/</guid><description>&lt;p&gt;In this blog post we are going to take a look at how to implement a simple CNN model from scratch in Python, using mostly just &lt;code&gt;numpy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In practice, we can use high-level libraries such as Keras or PyTorch to abstract away the underlying details of CNN when writing code. However, we find that the exercise of writing one from scratch is very helpful in gaining a deeper understanding of CNNs and how these frameworks work under the hood.&lt;/p&gt;</description></item></channel></rss>