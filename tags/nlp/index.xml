<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Bits&amp;Brains</title><link>https://sarckk.github.io/tags/nlp/</link><description>Recent content in NLP on Bits&amp;Brains</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 10 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://sarckk.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Grokking Transformers</title><link>https://sarckk.github.io/post/2023/04/10/grokking-transformers/</link><pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate><guid>https://sarckk.github.io/post/2023/04/10/grokking-transformers/</guid><description>Today&amp;rsquo;s Large Language Models (LLM) are based on Transformers, a deep learning model architecture for sequence-to-sequence transformations based on the attention mechanism. While it was originally proposed and used in Natural Language Processing (NLP) tasks like language translation, it turns out that a lot of things that we care about can be modelled in terms of sequences, making transformers a useful model in a wide variety of applications beyond NLP such as image processing and reinforcement learning .</description></item></channel></rss>