<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Grokking Transformers (WIP) | Shin</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/base16/gruvbox-dark-medium.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/highlight.min.js></script>
<script>hljs.highlightAll()</script></head><body><a href=/ style=all:initial;cursor:pointer><h1>Bits&Brains</h1></a><hr><div class=article-meta><h1><span class=title>Grokking Transformers (WIP)</span></h1><h2 class=date>2023/04/10</h2><p class=terms>Tags: <a href=/tags/ai>AI</a> <a href=/tags/llm>LLM</a></p></div><main><p>Today&rsquo;s Large Language Models (LLM) are based on Transformers, a deep learning model architecture for sequence-to-sequence transformations based on the attention mechanism. While it was originally proposed and used in Natural Language Processing (NLP) tasks like language translation, it turns out that a lot of things that we care about can be modelled in terms of sequences, making transformers a useful model in a wide variety of applications beyond NLP, such as <a href>image processing</a>
and <a href>reinforcement learning</a>
. Given the overwhelming success of transformers in deep learning and the outsized impact that transformer-based generative AI (e.g. GPT) has had &ndash; and will likely continue to have &ndash; on our society, I thought I should finally take time to read and understand the paper <a href=https://arxiv.org/abs/1706.03762 target=_blank rel="noreferrer noopener">&ldquo;Attention Is All You Need&rdquo; (Vaswani et al., 2017)</a>
that first proposed Transformers. That paper is now almost 6 years old(!) but better late than never, right?</p><p>There are already many writings covering transformers on the internet, so this article is mostly for my own learning &ndash; this is already <a href=https://ideas.time.com/2011/11/30/the-protege-effect/ target=_blank rel="noreferrer noopener">well documented</a>
, but I find that writing in a pedagogical style helps immensely in solidifying in my learnings and is almost always worth the effort. If anyone else stumbles across this post and finds it helpful, that&rsquo;s an added bonus!</p><p>This post will cover the technical details behind the Transformer model. The core concept behind transformers &ndash; self- and cross- attention &ndash; really isn&rsquo;t too hard too grasp, but I&rsquo;ve found that actually getting your hands dirty and implementing the model in Pytorch elevates your understanding of the material. Personally, I ran into many issues while trying to write and train the model that I wouldn&rsquo;t have known had I stopped at reading the paper or other tutorials online.</p><h2 id=all-about-transformations>All about transformations</h2><p>A Transformer &ndash; as its name suggests &ndash; <em>transforms</em> an input sequence $(x_1,x_2,&mldr;,x_n)$ into an output sequence $(y_1,y_2,&mldr;,y_m)$. Because this formulation is so general, it doesn&rsquo;t say what $x_1$ and $y_1$ should represent &ndash; it could be a word, a sub-word, a character, a pixel, or a token representing any arbitrary thing. However, I&rsquo;ll be talking about Transformers in the context of NLP here, because that&rsquo;s what it was originally invented for. So if we&rsquo;re talking about machine translation, the input sequence could be a sequence of words in one language (e.g. Korean) and the output could be a sequence of words in the target language (e.g. English):</p><p><img src=https://sarckk.github.io/media/transformer_1.svg alt></p><p>In the diagram above, each element in a sequence represents a word for simplicity, but in practice, it is common for this to a smaller unit than a word, like a subword for example. This depends on the tokenizer you use.</p><h2 id=diving-into-transformers>Diving into Transformers</h2><p>Now let&rsquo;s talk about what a Transformer actually looks like. From the original paper:</p><p align=center width=100%><img src=https://sarckk.github.io/media/transformer_architecture.png width=450/></p><p>If you&rsquo;re like me, this might be a bit overwhelming to take in at first. In reality, there&rsquo;s only 4 major components to a transformer architecture: the embedding layer, the encoder, the decoder, and the final linear+softmax layers that transform the output of the decoder into probabilities. Here&rsquo;s the same diagram with some annotations overlaid on top:</p><p align=center width=100%><img src=https://sarckk.github.io/media/transformer_arch_illustrated.png width=400/></p><p>At a high level, here&rsquo;s the journey that our input sequence takes to be transformed into an output sequence:</p><ol><li>Go through input embedding layer which projects each element in a sequence $x_i$ into a higher dimensional vector.</li><li>Add &ldquo;Positional Encoding&rdquo; vector to each element in the sequence (which remember, is now a high-dimensional vector). We&rsquo;ll talk about this in more detail later.</li><li>Go through the Encoder (orange block in the diagram above) <strong>N</strong> times. These <strong>N</strong> encoders have the same architecture but do not share weights.At the end of this step, we get a tensor that compactly represents the input sequence.</li><li>On the decoder side, we pass in a sequence of length <strong>M</strong>, which goes through the same embedding layer + positional encoding as we had for the input sequence.</li><li>The output embedding goes through a stack of <strong>N</strong> decoders (again no sharing of weights), each of which uses the tensor we got from <strong>Step 3</strong> in some way. At the end of this step, we get another tensor.</li><li>We pass this tensor through a final Linear + Softmax layer to obtain <strong>M</strong> probabilities, where again <strong>M</strong> is the length of the sequence we passed into the decoder.</li><li>We convert those probabilities into actual tokens (e.g. by taking the token with the highest probability), giving us an output sequence of length <strong>M</strong>.</li></ol><p>So from an input sequence of length <strong>N</strong> and decoder input of length <strong>M</strong>, we got &ndash; as the final product of the Transformer &ndash; another sequence of length <strong>M</strong>.</p><p>It&rsquo;s okay if some of these steps do not make sense yet, especially on why we need a sequence of length <strong>M</strong> to pass into the decoder to get another sequence of the same length as the output. I was initially confused by this as well: if we are just passing in an input sentence like "" and expect the model to output the translated text, what are we passing into the decoder? WTF? Don&rsquo;t worry, this will become clear when we talk about Transformers at training and inference time, later in this article.</p><p>Now, let&rsquo;s talk about each of these components in greater detail during <strong>training</strong>. Then we&rsquo;ll talk about what happens at <strong>inference time</strong>.</p><h2 id=diving-deeper-the-embedding-layer>Diving deeper: the embedding layer</h2><p>Let&rsquo;s start from the very beginning. The original Transformer in the 2017 paper was trained on the task of translating English sentences to German, using the WMT 2014 English-German dataset. This dataset contains ~4.5 million pairs of English sentence and its corresponding translation in German. We&rsquo;ll use this example to explain what happens in a Transformer for the rest of the article.</p><p>This means that during training, the input to the transformer (bottom left of Figure 1 above, below the very first decoder) is a sequence(s) of tokens in the English sentence. The paper mentions that they used <a href=https://en.wikipedia.org/wiki/Byte_pair_encoding target=_blank rel="noreferrer noopener">byte-pair</a>
encoding (page 7) to tokenize the sentences, but for simplicity I&rsquo;ll assume that the sentences are tokenized word by word:</p><p><img src=https://sarckk.github.io/media/seq_len_input.svg alt></p><p>Above is an illustration of what this would look like for a mini-batch of 3 English sentences. Note that we have 3 special tokens: <code>&lt;BOS></code> denoting the beginning of sentence, <code>&lt;EOS></code> marking the end of a sentence, and <code>&lt;PAD></code> representing an &ldquo;empty&rdquo; token to make all the sequences in the tensor of the same length (i.e. the maximum sequence length across all sentences in the mini-batch).</p><h3 id=step-1-string---number>Step 1: String -> Number</h3><p>Before we can pass this into the encoder, we need to convert these sequences of strings into a numerical representation instead so we can do some computation and make GPUs go <em>brrr</em>. To do this, we create &ndash; from the dataset &ndash; a mapping from all possible tokens (in this case, words) to its index in a vocabulary. So, the word &ldquo;The&rdquo; might have an index of 37, which uniquely identifies that token. We also leave out some indices for the special tokens that we introduced (<code>&lt;BOS></code>, <code>&lt;EOS></code> and <code>&lt;PAD></code>): for example, their indices might be 0, 1 and 2 respectively.</p><h3 id=step-2-number---embedding-vector>Step 2: Number -> Embedding Vector</h3><p>Once we have the input tensor, we turn each word (which by this point is a number representing its index in our vocabulary) into a high-dimensional vector that we call an <strong>embedding vector</strong>. In the original paper, the embedding vector is <strong>512-dimensional</strong>, but this is a hyperparameter that we can tune for our model through experiments.</p><p>After this step, we have a tensor of shape (batch size, (max) input sequence length, embedding dimension). For the rest of the article, I&rsquo;ll use the short form <code>B</code> for batch size, <code>T</code> for the maximum input sequence length, and <code>D</code> for the embedding dimension. In our example, <code>B=3</code>, <code>T=9</code> and <code>D=512</code>.</p><h3 id=step-3-adding-positional-encoding>Step 3: Adding Positional Encoding</h3><p>We&rsquo;ll actually skip this step for now, because it only really makes sense when we start looking at the Encoder and self-attention. Without too much detail, though, here we basically add a tensor with the same shape <code>(B,T,D)</code> to our tensor from <strong>Step 2</strong>. This tensor that we add encodes information about the relative order of each word in a sentence, since this is information that we&rsquo;d like the Transformer to consider in the computation of the final output probabilities. We&rsquo;ll come back to this shortly.</p><p>In the end, the input to the Encoder is a tensor of shape <code>(B,T,D)</code>. The same thing happens for the Decoder, except with German sentences in our example of English-to-German translation.</p><h2 id=encoder>Encoder</h2><p>We&rsquo;ve finally reached the Encoders. The encoding step consists of <strong>N</strong> independent Encoder units stacked on top of each other. These Encoders are identical in architecture, but do not share weights between them and are thus separately updated during backpropagation.</p><p>Let&rsquo;s zoom in to see what it&rsquo;s made of:</p><p align=center width=100%><img src=https://sarckk.github.io/media/encoder_here.png width=300/></p><p>The Encoder unit itself comprises 2 parts:</p><ol><li>First, Self-attention with multiple heads (Mult-Head Attention). This is followed by</li><li>A Feed Forward network.</li></ol><h3 id=attention-is-all-you-need>Attention is all you need</h3><p>The <strong>core idea</strong> behind the Transformer is to replace recurrence and convolutions that made up previous sequence-to-sequence models with one entirely based on the attention mechanism. In simple terms, the attention mechanism is basically just taking a bunch of dot products between sequences. And <strong>self-attention</strong> is just particular case of attention where the sequences that we&rsquo;re concerned with is actually all the same &ndash; just one sequence.</p><p>Remember that at this point, our example input to the Encoder is a tensor of shape <code>(3,9,512)</code>. To make the explanation easier, let&rsquo;s look at what happens for <strong>one</strong> single sentence out of 3 total sentences in this mini-batch: when we actually pass through the entire batch with 3 sentences, logically it will be as if we pass through each sentence separately and merge the 3 outputs together.</p><p>Let&rsquo;s look at just one sentence: &ldquo;This jacket is too small for me&rdquo;. After the embedding layer, we have a tensor of shape <code>(9,512)</code>. To encode this tensor, we essentially pass all 9 of the 512-dimensional embedding vector to the self-attention module <strong>at the same time</strong>:</p><p><img src alt></p><p>The goal of the self-attention module is to figure out how the words in the sentence (or more generally, tokens in a sequence) relate to each other.</p><p>Remember, the sentence is &ldquo;This jacket is too small for me&rdquo;. When we look at the adjective <code>"small"</code>, we want to understand what object it is referring to. Clearly, we know that it is referring to the <code>"jacket"</code>, but the Transformer model has to learn this. In other words, it has to learn how each word relates to another. In vector space, we have a concept for computing the similarity between vectors: <strong>dot product</strong>.</p><p>Dot products form the basis of the attention mechanism.</p><h3 id=the-attention-mechanism>The Attention mechanism</h3><p>The attention mechanism involves 3 components: query(s), keys(s), and value(s) where these are all vectors. More formally, we have:</p><ul><li>Queries $q_1,&mldr;,q_T$ where $q_i \in$ $\R^{d_k}$, where $d_k$ is the dimension of the query vector, and $T$ is the number of queries</li><li>Keys $k_1,&mldr;k_K$ where $k_i \in$ $\R^{d_k}$, and $K$ is the number of key-value pairs</li><li>Value $v_1,&mldr;,v_K$ where $v_i \in$ $\R^{d_v}$, where $d_v$ is the dimension of the value vector, which is not necessarily equal to $Q$, although in our case of English-German translation, it is.</li></ul><p>Note that $T$, the number of queries doesn&rsquo;t necessarily have to equal $K$, the number of key-value pairs, but the number of keys must be the same as the number of values (for them to form a key-value pair). Furthermore, the query and key vectors must have the same dimension, $d_k$ so we can do a dot product.</p><p>Given this formulation, the attention function on $q_i$ does the following:</p><ul><li>Get dot product of query and key vectors to get a scalar value: $\alpha_{ij} = q_i \cdot k_i$</li><li>Normalize each dot product $\alpha_{ij}$ by performing <a href=https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax target=_blank rel="noreferrer noopener">softmax</a>
across all $j$, where $j=0,&mldr;,K$ , where $K$ is the number of key-value pairs. This gives us weights $w_{ij}$ for all $j$.</li><li>Output weighted sum of all $v_j$ where weight of $v_j$ is $w_{ij}$.</li></ul><h3 id=back-to-our-example>Back to our example</h3><p>Back to our example sentence, we have $x_1,&mldr;,x_9$ where $x_i$ is a 512-dimensional embedding vector representing each word in the sentence <code>"This jacket is too small for me"</code> plus the <code>&lt;BOS></code> and <code>&lt;EOS></code> tokens. We obtain our query, key and value vectors from $x_i$ by multiplying it each time with a different matrix:</p><p>\begin{equation}
k_i = Kx_i, where K is a d_k \times d_k matrix.
\end{equation}
\begin{equation}
q_i = Qx_i, where Q is a d_k \times d_k matrix.
\end{equation}
\begin{equation}
v_i = Vx_i, where V is a d_v \times d_v matrix.
\end{equation}</p><p>In our case, $d_k=d_v=512$. It&rsquo;s important that $K$, $Q$ and $V$ are separate matrices (and therefore independently traininable) because this allows for more flexibility in both how the model chooses to define &ldquo;similarity&rdquo; between words (by updating $K$ and $Q$), as well as what the final weighted sum represents (by updating $V$) in latent space. In Pytorch code, these matrices are implemented as <code>nn.Linear()</code> modules with <code>bias=False</code>.</p><p>Now that we have $k_i$, $q_i$ and $v_i$, we just compute the corresponding output for $x_i$ using the steps outlined earlier, computing the sum of all vectors weighed by the dot products. Here, since $q_i$,$k_i$ and $v_i$ are all derived from $x_i$, we give it a special name: <strong>self-attention</strong>.</p><p><img src alt="Illustratiotion of self-attention"></p><h3 id=parallelizing-attention-computation>Parallelizing attention computation</h3><p>As you can see, attention is computed using dot products between any two words within a sequence, allowing the Transformer to learn long-range dependencies in a sequence more easily. One downside of this, though, is that the computation of attention scores is quadratic in the length of the input sequence $N$. This quadratic $O(N^2)$ complexity is an issue because it means it will take a lot of compute for long sequences.</p><p>Fortunately, we can represent the computation as a product of a few matrix multiplications, which is easily parallelizable on GPU/TPUs.</p><p>In matrix form, we can formulate attention as the interaction between $Q$, $K$ and $V$ matrices for queries, keys and values respectively.</p><p>In short, we can formulate the attention mechanism described above as:</p><p>\begin{equation}
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}</p><p>This is the standard matrix formulation of attention that the authors use in the Transformers paper (page 4). Notice that we are scaling the matrix ${QK_T}$ (i.e. the dot-product between all rows of query-matrix $Q$ with all columns of key-matrix $K$) by $\frac{1}{d_k}$, to prevent dot products from becoming too large, &ldquo;pushing the softmax function into regions where it has extremely small gradients&rdquo; (<a href=https://arxiv.org/pdf/1706.03762.pdf target=_blank rel="noreferrer noopener">Viswani et al, 2017, pg 4</a>
).</p><p>This ability for parallelization is a part of why the Transformer has been so successful &ndash; previous models based on recurrence, for example, cannot be parallelized because the computation of its state at time $t$, $h_t$ necessarily depends on the computation of its previous state at time $t-1$, $h_{t-1}$.</p><h3 id=multiple-heads>Multiple Heads</h3><p>Let&rsquo;s talk about multi-head attention &ndash; why the need for multiple heads?</p><h3 id=back-to-positional-encodings>Back to Positional Encodings</h3><p>Now we can finally talk about why we need positional encodings. We&rsquo;ve seen that (self-)attention basically comes down to taking a bunch of dot products and outputting a new vector with this information. The problem is, by simply taking dot products, we lose information about the relative order of these words in a sentence. We know that the position of a word in a sentence matters a great deal. For example, consider the sentence:</p><p>and the sentence:</p><p>These are similar but the relative order of the word <code>""</code> completely changes the meaning. We&rsquo;d like the Transformer to</p><details><summary>Implementation of Positional Encoding layer</summary>
There are many ways to implement this, but I've chosen to do it this way:<pre><code class=language-python>class PositionalEncoding(nn.Module):
  def __init__(self, emb_dim: int, max_seq_len: int = 5000):
    super().__init__()
    assert emb_dim % 2 == 0, &quot;Embedding dimension must be divisble by 2&quot;
    self.dropout = nn.Dropout(0.1)
    
    pos = torch.arange(max_seq_len)[:, None] # [seq_len, 1]
    evens = 10000. ** (-torch.arange(0,emb_dim,step=2) / emb_dim)
    evens = evens[None, :] # [1, ceil(emb_dim/2)]
    evens = pos * evens # [seq_len, ceil(emb_dim/2)]
    pe = rearrange([evens.sin(), evens.clone().cos()], 't h w -&gt; h (w t)') # interleave even and odd parts
      
    self.register_buffer('pe', pe) # [max_seq_len, emb_dim]
  
  def forward(self, 
              src: Tensor # [bsz, seq_len, emb_dim]
              ) -&gt; Tensor:
    assert src.shape[-1] == self.pe.shape[1], f&quot;Expected embedding dimension of {self.pe[1]} but got {src.shape[-1]} instead.&quot;
    out = src + self.pe[None,:src.size(1),:]
    return self.dropout(out) # See Page 7 of original paper, under section &quot;Regularization&quot;

  def check_and_plot_pos_encoding(self): 
    # modified from tensorflow example (https://www.tensorflow.org/text/tutorials/transformer#positional_encoding)

    # Check the shape.
    print(self.pe.numpy().shape)

    # Plot the dimensions.
    plt.pcolormesh(self.pe.numpy(), cmap='RdBu')
    plt.gca().invert_yaxis()
    plt.xlabel('Depth')
    plt.ylabel('Position')
    plt.colorbar()
    plt.show()
</code></pre><p>Note that the <code>self.register_buffer('pe', pe)</code> line is important because while the positional encodings do not have trainable parameters, this adds the encoding to the model&rsquo;s parameters and ensures that it is saved during <code>torch.save()</code>.</p></details><h3 id=masking>Masking</h3><p>TODO</p><h3 id=feed-forward-networks>Feed Forward Networks</h3><p>Adding nonlinearity, TODO</p><h3 id=residual-connections--layer-normalization>Residual Connections + Layer Normalization</h3><p>TODO</p><h2 id=decoder>Decoder</h2><p>Phew! There was quite a lot to cover for Encoders. Fortunately, I&rsquo;ve already covered most of the important parts of the Transformer, and the decoding part more or less mimics what we had in the encoding phase.</p><p>Similar to encoding, we have $N$ Decoder modules &ndash; all independently trained. Each Decoder is similar to the Encoder, except we have an additional stage of Attention computation + feed forward network. The difference is that this attention is called <strong>cross-attention</strong>, where we use the final encoder output (remember this is a tensor of shape <code>(B,T,D)</code>) as the key and value tensors for attention, while the queries come from the decoder itself (input for the first decoder, and the output of the previous decoder for subsequent decoders).</p><h3 id=cross-attention>Cross-Attention</h3><p>Cross-attention is simlar, except we use (TODO)</p><h3 id=masking-1>Masking</h3><p>Masking is slightly different for Decoders, too. Instead of&mldr;</p><h2 id=conclusion>Conclusion</h2><p>TODO</p><p>The code for the article can be found on my <a href=https://github.com/sarckk/transformer_series/blob/main/transformers.ipynb target=_blank rel="noreferrer noopener">github</a>
. My aim was to get the training working as quickly as possible so it&rsquo;s far from polished.</p></main><footer><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
</script><script defer src=//yihui.org/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//yihui.org/js/center-img.js></script><hr><a href=https://github.com/sarckk target=_blank rel="noreferrer noopener">Github</a></footer></body></html>