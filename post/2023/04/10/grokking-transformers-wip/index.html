<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Grokking Transformers (WIP) | Bits&amp;Brains</title><link rel=stylesheet href=/%20/css/style.css><link rel=stylesheet href=/%20/css/fonts.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/base16/gruvbox-dark-medium.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/highlight.min.js></script>
<script>hljs.highlightAll()</script><link type=text/css rel=stylesheet href=https://sarckk.github.io/css/jquery.tocify.css><script src=https://code.jquery.com/jquery-3.6.4.min.js integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin=anonymous></script>
<script src=https://sarckk.github.io/scripts/jquery.tocify.min.js></script>
<script>document.URL.includes("grokking-transformers")&&$(function(){$("#toc").tocify({selectors:"h1:not('.exclude-toc'),h2:not('.exclude-toc'),h3:not('.exclude-toc')"})})</script></head><body><a href=/ style=all:initial;cursor:pointer><h1 class=exclude-toc>Bits&Brains</h1></a><hr><div class=article-meta><h1 class=exclude-toc><span class=title>Grokking Transformers (WIP)</span></h1><h2 class=date>2023/04/10</h2><p class=terms>Tags: <a href=/tags/ai>AI</a> <a href=/tags/llm>LLM</a></p></div><div id=toc></div><main><p>Today&rsquo;s Large Language Models (LLM) are based on Transformers, a deep learning model architecture for sequence-to-sequence transformations based on the attention mechanism. While it was originally proposed and used in Natural Language Processing (NLP) tasks like language translation, it turns out that a lot of things that we care about can be modelled in terms of sequences, making transformers a useful model in a wide variety of applications beyond NLP, such as <a href=https://arxiv.org/abs/2103.14030 target=_blank rel="noreferrer noopener">image processing</a>
and <a href=https://arxiv.org/abs/2106.01345 target=_blank rel="noreferrer noopener">reinforcement learning</a>
. Given the overwhelming success of transformers in deep learning and the outsized impact that transformer-based generative AI (e.g. GPT) has had &ndash; and will likely continue to have &ndash; on our society, I thought I should finally take time to read and understand the paper <a href=https://arxiv.org/abs/1706.03762 target=_blank rel="noreferrer noopener">&ldquo;Attention Is All You Need&rdquo; (Vaswani et al., 2017)</a>
that first proposed Transformers. That paper is now almost 6 years old(!) but better late than never, right?</p><p>There are already <a href=https://jalammar.github.io/illustrated-transformer/ target=_blank rel="noreferrer noopener">many</a>
<a href="https://www.youtube.com/watch?v=n9sLZPLOxG8" target=_blank rel="noreferrer noopener">tutorials</a>
<a href="https://www.youtube.com/watch?v=n9sLZPLOxG8" target=_blank rel="noreferrer noopener">covering</a>
<a href="https://www.youtube.com/watch?v=ptuGllU5SQQ" target=_blank rel="noreferrer noopener">transformers</a>
online, so this article is mostly for my own learning &ndash; this is already <a href=https://ideas.time.com/2011/11/30/the-protege-effect/ target=_blank rel="noreferrer noopener">well documented</a>
, but I find that writing in a pedagogical style helps immensely in solidifying in my learnings and is almost always worth the effort. If anyone else stumbles across this post and finds it helpful, that&rsquo;s an added bonus!</p><p>This post will cover the technical details behind the Transformer model. The core concept behind transformers &ndash; self- and cross- attention &ndash; really isn&rsquo;t too hard too grasp, but I&rsquo;ve found that actually getting your hands dirty and implementing the model in Pytorch elevates your understanding of the material. Personally, I ran into many issues while trying to write and train the model that I wouldn&rsquo;t have known had I stopped at reading the paper or other tutorials online.</p><h1 id=table-of-contents----omit-from-toc--->Table of Contents</h1><ul><li><a href=#all-about-transformations>All about transformations</a></li><li><a href=#diving-into-transformers-the-architecture>Diving into Transformers: the architecture</a><ul><li><a href=#diving-deeper-the-embedding-layer>Diving deeper: the embedding layer</a><ul><li><a href=#step-1-string---number>Step 1: String -> Number</a></li><li><a href=#step-2-number---embedding-vector>Step 2: Number -> Embedding Vector</a></li><li><a href=#step-3-adding-positional-encoding>Step 3: Adding Positional Encoding</a></li></ul></li><li><a href=#encoder>Encoder</a><ul><li><a href=#attention-is-all-you-need>Attention is all you need</a></li><li><a href=#a-closer-look-at-attention>A closer look at attention</a></li><li><a href=#matrix-formulation-of-attention>Matrix formulation of attention</a></li><li><a href=#back-to-positional-encodings>Back to Positional Encodings</a></li><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#masking-in-the-encoder>Masking in the encoder</a></li><li><a href=#feed-forward-network>Feed-Forward Network</a></li><li><a href=#encoder-the-remaining-bits>Encoder: the remaining bits</a></li></ul></li><li><a href=#decoder>Decoder</a><ul><li><a href=#difference-1-cross-attention>Difference #1: Cross-Attention</a></li><li><a href=#difference-2-masking-in-self-attention>Difference #2: Masking in self-attention</a></li></ul></li><li><a href=#linear--softmax-layer>Linear + Softmax Layer</a></li></ul></li><li><a href=#training>Training</a><ul><li><a href=#loss-function>Loss function</a></li><li><a href=#regularization>Regularization</a></li><li><a href=#model-hyperparameters>Model hyperparameters</a></li><li><a href=#optimizer>Optimizer</a></li><li><a href=#learning-rate-scheduling>Learning-rate scheduling</a></li></ul></li><li><a href=#inference>Inference</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#acknowledgements>Acknowledgements</a></li><li><a href=#where-to-go-from-here>Where to go from here</a></li></ul><h1 id=all-about-transformations>All about transformations</h1><p>A Transformer &ndash; as its name suggests &ndash; <em>transforms</em> an input sequence $(x_1,x_2,&mldr;,x_n)$ into an output sequence $(y_1,y_2,&mldr;,y_m)$. Because this formulation is so general, it doesn&rsquo;t say what $x_1$ and $y_1$ should represent &ndash; it could be a word, a sub-word, a character, a pixel, or a token representing any arbitrary thing. However, I&rsquo;ll be talking about Transformers in the context of NLP here, because that&rsquo;s what it was originally invented for. So if we&rsquo;re talking about machine translation, the input sequence could be a sequence of words in one language (e.g. Korean) and the output could be a sequence of words in the target language (e.g. English):</p><p><img src=https://sarckk.github.io/media/transformer_1.svg alt></p><p>In the diagram above, each element in a sequence represents a word for simplicity, but in practice, it is common for this to a smaller unit than a word, like a subword for example. This depends on the tokenizer you use.</p><h1 id=diving-into-transformers-the-architecture>Diving into Transformers: the architecture</h1><p>Now let&rsquo;s talk about what a Transformer actually looks like. From the original paper:</p><p align=center width=100%><img src=https://sarckk.github.io/media/transformer_architecture.png width=450/></p><p>If you&rsquo;re like me, this might be a bit overwhelming to take in at first. In reality, there&rsquo;s only 4 major components to a transformer architecture: the embedding layer, the encoder, the decoder, and the final linear+softmax layers that transform the output of the decoder into probabilities. Here&rsquo;s the same diagram with some annotations overlaid on top:</p><p align=center width=100%><img src=https://sarckk.github.io/media/transformer_arch_illustrated.png width=400/></p><p>At a high level, here&rsquo;s the journey that our input sequence takes to be transformed into an output sequence:</p><ol><li>Go through input embedding layer which projects each element in a sequence $x_i$ into a higher dimensional vector.</li><li>Add &ldquo;Positional Encoding&rdquo; vector to each element in the sequence (which remember, is now a high-dimensional vector). We&rsquo;ll talk about this in more detail later.</li><li>Go through the encoder (orange block in the diagram above) <strong>N</strong> times. These <strong>N</strong> encoders have the same architecture but do not share weights.At the end of this step, we get a tensor that compactly represents the input sequence.</li><li>On the decoder side, we pass in a sequence of length <strong>M</strong>, which goes through the same embedding layer + positional encoding as we had for the input sequence.</li><li>The output embedding goes through a stack of <strong>N</strong> decoders (again no sharing of weights), each of which uses the tensor we got from <strong>Step 3</strong> in some way. At the end of this step, we get another tensor.</li><li>We pass this tensor through a final Linear + Softmax layer to obtain <strong>M</strong> probabilities, where again <strong>M</strong> is the length of the sequence we passed into the decoder.</li><li>We convert those probabilities into actual tokens (e.g. by taking the token with the highest probability), giving us an output sequence of length <strong>M</strong>.</li></ol><p>So from an input sequence of length <strong>N</strong> and decoder input of length <strong>M</strong>, we got &ndash; as the final product of the Transformer &ndash; another sequence of length <strong>M</strong>.</p><p>It&rsquo;s okay if some of these steps do not make sense yet, especially on why we need a sequence of length <strong>M</strong> to pass into the decoder to get another sequence of the same length as the output. I was initially confused by this as well: if we are just passing in an input sentence like "" and expect the model to output the translated text, what are we passing into the decoder? WTF? Don&rsquo;t worry, this will become clear when we talk about Transformers at training and inference time, later in this article.</p><p>Now, let&rsquo;s talk about each of these components in greater detail during <strong>training</strong>. Then we&rsquo;ll talk about what happens at <strong>inference time</strong>.</p><h2 id=diving-deeper-the-embedding-layer>Diving deeper: the embedding layer</h2><p>Let&rsquo;s start from the very beginning. The original Transformer in the 2017 paper was trained on the task of translating English sentences to German, using the WMT 2014 English-German dataset. This dataset contains ~4.5 million pairs of English sentence and its corresponding translation in German. We&rsquo;ll use this example to explain what happens in a Transformer for the rest of the article.</p><p>This means that during training, the input to the transformer (bottom left of Figure 1 above, below the very first decoder) is a sequence(s) of tokens in the English sentence. The paper mentions that they used <a href=https://en.wikipedia.org/wiki/Byte_pair_encoding target=_blank rel="noreferrer noopener">byte-pair</a>
encoding (page 7) to tokenize the sentences, but for simplicity I&rsquo;ll assume that the sentences are tokenized word by word:</p><p><img src=https://sarckk.github.io/media/seq_len_input.svg alt></p><p>Above is an illustration of what this would look like for a mini-batch of 3 English sentences. Note that we have 3 special tokens: <code>&lt;BOS></code> denoting the beginning of sentence, <code>&lt;EOS></code> marking the end of a sentence, and <code>&lt;PAD></code> representing an &ldquo;empty&rdquo; token to make all the sequences in the tensor of the same length (i.e. the maximum sequence length across all sentences in the mini-batch).</p><h3 id=step-1-string---number>Step 1: String -> Number</h3><p>Before we can pass this into the encoder, we need to convert these sequences of strings into a numerical representation instead so we can do some computation and make GPUs go <em>brrr</em>. To do this, we create &ndash; from the dataset &ndash; a mapping from all possible tokens (in this case, words) to its index in a vocabulary. So, the word &ldquo;The&rdquo; might have an index of 37, which uniquely identifies that token. We also leave out some indices for the special tokens that we introduced (<code>&lt;BOS></code>, <code>&lt;EOS></code> and <code>&lt;PAD></code>): for example, their indices might be 0, 1 and 2 respectively.</p><h3 id=step-2-number---embedding-vector>Step 2: Number -> Embedding Vector</h3><p>Once we have the input tensor, we turn each word (which by this point is a number representing its index in our vocabulary) into a high-dimensional vector that we call an <strong>embedding vector</strong>. This can be implemented using PyTorch&rsquo;s <code>nn.Embedding()</code> module. In the original paper, the embedding dimension is <strong>512</strong>, but this is a hyperparameter that we can tune for our model through experiments. Note that in the paper, they also multiply the embedding weights by <code>sqrt</code> of the embedding dimension (see page 5).</p><p>After this step, we have a tensor of shape (batch size, (max) input sequence length, embedding dimension). For the rest of the article, I&rsquo;ll use the short form <code>B</code> for batch size, <code>T</code> for the maximum input sequence length, and <code>D</code> for the embedding dimension. In our example, <code>B=3</code>, <code>T=9</code> and <code>D=512</code>.</p><h3 id=step-3-adding-positional-encoding>Step 3: Adding Positional Encoding</h3><p>I&rsquo;ll skim over this step for now, because it only really makes sense when we start looking at how attention is computed. Without too much detail though, here we basically add a tensor with the same shape <code>(B,T,D)</code> to our tensor from <strong>Step 2</strong>. This tensor that we add encodes information about the relative order of each word in a sentence, since this is information that we&rsquo;d like the Transformer to consider in the computation of the final output probabilities. Again, I&rsquo;ll come back to this later in the article.</p><p>In the end, the input to the encoder is a tensor of shape <code>(B,T,D)</code>. The same thing happens for the decoder, except with German sentences in our example of English-to-German translation.</p><hr><h2 id=encoder>Encoder</h2><p>We&rsquo;ve finally reached the encoder. The encoding step consists of <strong>N</strong> independent encoder units stacked on top of each other. These encoders are identical in architecture, but do not share weights between them and are thus separately updated during backpropagation.</p><p>The encoder unit itself comprises 2 parts:</p><ol><li>A Self-Attention module, followed by</li><li>A Feed-Forward network.</li></ol><p>We&rsquo;ll start with this high level picture of the encoder and gradually fill in more details:</p><p align=center width=100%><img src=https://sarckk.github.io/media/encoder_here.png width=300/></p><h3 id=attention-is-all-you-need>Attention is all you need</h3><p>The <strong>core idea</strong> behind the Transformer is to replace recurrence and convolutions that made up previous sequence-to-sequence models with one entirely based on the attention mechanism. In simple terms, the attention mechanism is basically just taking a bunch of dot products between sequences. And <strong>self-attention</strong> is just particular case of attention where the sequences that we&rsquo;re concerned with is actually all the same &ndash; just one sequence.</p><p>Remember that at this point, our example input to the encoder is a tensor of shape <code>(3,9,512)</code>. To make the explanation easier, let&rsquo;s look at what happens for <strong>one</strong> single sentence out of 3 total sentences in this mini-batch: when we actually pass through the entire batch with 3 sentences, logically it will be as if we pass through each sentence separately and merge the 3 outputs together.</p><p>Let&rsquo;s look at just one sentence: &ldquo;This jacket is too small for me&rdquo;. After the embedding layer, we have a tensor of shape <code>(9,512)</code>. To encode this tensor, we essentially pass all 9 of the 512-dimensional embedding vector to the self-attention module <strong>at the same time</strong>:</p><p><img src alt="TODO: insert illustration here"></p><p>The goal of the self-attention module is to figure out how the words in the sentence (or more generally, tokens in a sequence) relate to each other.</p><p>Remember, the sentence is &ldquo;This jacket is too small for me&rdquo;. When we look at the adjective <code>"small"</code>, we want to understand what object it is referring to. Clearly, we know that it is referring to the <code>"jacket"</code>, but the Transformer model has to learn this. In other words, it has to learn how each word relates to another. In vector space, we have a concept for computing the similarity between vectors: <strong>dot product</strong>.</p><p>Dot products form the basis of the attention mechanism.</p><h3 id=a-closer-look-at-attention>A closer look at attention</h3><p>Computing attention involves 3 inputs: query(s), keys(s), and value(s) where these are all vectors. More formally, we have:</p><ul><li>Queries $q_1,&mldr;,q_T$ where $q_i \in$ $\R^{d_k}$, where $d_k$ is the dimension of the query vector, and $T$ is the number of queries</li><li>Keys $k_1,&mldr;k_K$ where $k_i \in$ $\R^{d_k}$, and $K$ is the number of key-value pairs</li><li>Value $v_1,&mldr;,v_K$ where $v_i \in$ $\R^{d_v}$, where $d_v$ is the dimension of the value vector, which is not necessarily equal to $Q$, although in our case of English-German translation, it is.</li></ul><p>Note that $T$, the number of queries doesn&rsquo;t necessarily have to equal $K$, the number of key-value pairs, but the number of keys must be the same as the number of values (for them to form a key-value pair). Furthermore, the query and key vectors must have the same dimension, $d_k$ so we can do a dot product.</p><p>Given this formulation, the attention function on $q_i$ does the following:</p><ul><li>Get dot product of query and key vectors to get a scalar value: $\alpha_{ij} = q_i \cdot k_i$</li><li>Normalize each dot product $\alpha_{ij}$ by performing <a href=https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax target=_blank rel="noreferrer noopener">softmax</a>
across all $j$, where $j=0,&mldr;,K$ , where $K$ is the number of key-value pairs. This gives us weights $w_{ij}$ for all $j$.</li><li>Output weighted sum of all $v_j$ where weight of $v_j$ is $w_{ij}$.</li></ul><p><strong>Back to our example sentence</strong>, we have $x_1,&mldr;,x_9$ where $x_i$ is a 512-dimensional embedding vector representing each word in the sentence <code>"This jacket is too small for me"</code> plus the <code>&lt;BOS></code> and <code>&lt;EOS></code> tokens. We obtain our query, key and value vectors from $x_i$ by multiplying it each time with a different matrix:</p><p>$$\begin{aligned}
k_i = W^Kx_i, \text{where } W^K \in \R^{d_k \times d_k} \
q_i = W^Qx_i, \text{where } W^Q \in \R^{d_k \times d_k} \
v_i = W^Vx_i, \text{where } W^V \in \R^{d_v \times d_v} \
\end{aligned}$$</p><p>In our case, $d_k=d_v=512$. We have $W^K$, $W^Q$ and $W^Q$ matrices that linearly project each key, query and value vectors &ndash; this allows for more flexibility in both how the model chooses to define &ldquo;similarity&rdquo; between words (by updating $K$ and $Q$), as well as what the final weighted sum represents (by updating $V$) in latent space. In Pytorch code, these matrices are implemented as <a href=https://pytorch.org/docs/stable/generated/torch.nn.Linear.html target=_blank rel="noreferrer noopener"><code>nn.Linear</code></a>
modules with <code>bias=False</code>.</p><p>Now that we have $k_i$, $q_i$ and $v_i$, we just compute the corresponding output for $x_i$ using the steps outlined earlier, computing the sum of all vectors weighed by the dot products. Here, since $q_i$,$k_i$ and $v_i$ are all derived from $x_i$, we give it a special name: <strong>self-attention</strong>.</p><p align=center width=100%><img src>
Illustration of self-attention here</p><h3 id=matrix-formulation-of-attention>Matrix formulation of attention</h3><p>As you can see, attention is computed using dot products between any two words within a sequence, allowing the Transformer to learn long-range dependencies in a sequence more easily. One downside of this, though, is that the computation of attention scores is quadratic in the length of the input sequence $N$. This quadratic $O(N^2)$ complexity is an issue because it means it will take a lot of compute for long sequences.</p><p>Fortunately, we can represent the computation as a product of a few matrix multiplications, which is easily parallelizable on GPU/TPUs. Given matrices $Q$, $K$ and $V$ containing rows of query, key and value vectors respectively, the general formulation of attention in matrix form is as follows:</p><p>\begin{equation}
Attention(Q,K,V) = softmax(QK^T)V
\end{equation}</p><p>Again, the $Q$, $K$ and $V$ matrices are computed using the corresponding weight matrices $W^Q$, $W^K$, and $W^V$: for example, if we have a matrix $X$ where each row is an embedding vector in our sequence, then we&rsquo;d have $Q=XW^Q$, $K=XW^K$ and $V=XW^V$ for the self-attention sublayer in the encoder. As we&rsquo;ll see later when we get to cross-attention in the decoder, $Q$, $K$ and $V$ do not necessarily need to come from the same single matrix $X$.</p><p>The authors in the Transformers paper also apply a scaling factor of $\frac{1}{d_k}$ to the matrix of dot products (numerator) to prevent the products from becoming too large, which can &ldquo;[push] the softmax function into regions where it has extremely small gradients&rdquo; (<a href=https://arxiv.org/pdf/1706.03762.pdf target=_blank rel="noreferrer noopener">Viswani et al, 2017, pg 4</a>
):</p><p>\begin{equation}
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}</p><p>This ability for parallelization is a part of why the Transformer has been so successful &ndash; previous models based on recurrence, for example, cannot be parallelized because the computation of its state at time $t$, $h_t$ necessarily depends on the computation of its previous state at time $t-1$, $h_{t-1}$.</p><h3 id=back-to-positional-encodings>Back to Positional Encodings</h3><p>Now we can finally talk about why we need positional encodings. We&rsquo;ve seen that (self-)attention basically comes down to taking a bunch of dot products and outputting a new vector with this information. The problem is, by simply taking dot products, we lose information about the relative order of these words in a sentence. And we know that the position of a word in a sentence matters.</p><p>To encode information about the position of each token in the sequence, we add <strong>positional encodings</strong> to the input embeddings. In practice, there are many ways to generate this &ndash; including having the network learn this during training &ndash; but the authors use the following formula:</p><p>$$\begin{aligned}
PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{emb}}) \
PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{emb}})
\end{aligned}$$</p><p>where $i$ is the index along the embedding dimension and $pos$ is the position of the token in the sequence. Both are 0-indexed. By having sine and cosine functions of varying periods, we are able to inject information about position in continuous form.</p><p align=center style=display:flex;flex-direction:column;align-items:center><img src=https://sarckk.github.io/media/pos_encoding.png width=400><caption>Illustration of positional encodings</caption></p><p>I don&rsquo;t have much to add on positional encodings, though I&rsquo;ll point out that the periodic nature of sinusoids used here has some nice properties, like placing more emphasis on <strong>relative</strong> &ndash; as opposed to absolute &ndash; order.</p><details><summary>Implementation of Positional Encoding layer</summary>
There are many ways to implement this, but I've chosen to do it this way:<pre><code class=language-python>class PositionalEncoding(nn.Module):
  def __init__(self, emb_dim: int, max_seq_len: int = 5000):
    super().__init__()
    assert emb_dim % 2 == 0, &quot;Embedding dimension must be divisble by 2&quot;
    self.dropout = nn.Dropout(0.1)
    
    pos = torch.arange(max_seq_len)[:, None] # [seq_len, 1]
    evens = 10000. ** (-torch.arange(0,emb_dim,step=2) / emb_dim)
    evens = evens[None, :] # [1, ceil(emb_dim/2)]
    evens = pos * evens # [seq_len, ceil(emb_dim/2)]
    pe = rearrange([evens.sin(), evens.clone().cos()], 't h w -&gt; h (w t)') # interleave even and odd parts
      
    self.register_buffer('pe', pe) # [max_seq_len, emb_dim]
  
  def forward(self, 
              src: Tensor # [bsz, seq_len, emb_dim]
              ) -&gt; Tensor:
    assert src.shape[-1] == self.pe.shape[1], f&quot;Expected embedding dimension of {self.pe[1]} but got {src.shape[-1]} instead.&quot;
    out = src + self.pe[None,:src.size(1),:]
    return self.dropout(out) # See Page 7 of original paper, under section &quot;Regularization&quot;
</code></pre><p>Note that the <code>self.register_buffer('pe', pe)</code> line is important because while the positional encodings do not have trainable parameters, this adds the encoding to the model&rsquo;s parameters and ensures that it is saved during <code>torch.save()</code>.</p></details><br><h3 id=multi-head-attention>Multi-Head Attention</h3><p>In the paper, the authors use <strong>Multi-Head Attention (MHA)</strong>. In MHA we have multiple &ldquo;heads&rdquo; that each performs the attention computation that we <em>just</em> talked about. However, each head $h_i$ has its own linear projection matrices $K_i$, $Q_i$, and $V_i$, and these matrices project the key, query and value vectors to a <strong>lower</strong> dimensional space than we had with single matrices.</p><p>For example, if the dimension of matrix $K$ in <strong>Single-Head Attention</strong> was $512 \times 512$, then the dimension of $K_1$ and $K_2$ in a <strong>2-Head Attention</strong> would each be $512 \times 256$, thus projecting to a 256-dimensional space instead of 512-dimensional.</p><p>After all the heads compute its own value of <code>Attention(X,Q_i,K_i,V_i)</code> in parallel, we concatenate the outputs to obtain an output of the same shape as we had in the case of single-head attention. This is followed by a final linear projection to $d_{emb}$-dimensional space where $d_{emb}$ is the embedding dimension. For our example input of shape <code>(9,512)</code>, MHA produces an output of the same shape.</p><p>The intuition behind why having multiple heads improves performance is that by having independently trainable linear projections per head, the model is able to simultaneously attend to different aspects of the language (for example, for a model trained on LaTeX documents, it might have one head that learns to attend to a presence of a <code>\end</code> command if a <code>\begin</code> command appears in the sequence, and another head that relates words in terms of their semantic relevance in text).</p><details><summary>PyTorch implementation of Mult-Head Attention</summary><pre><code class=language-python>class MultiHeadAttention(nn.Module):
  def __init__(self,
               emb_dim: int,
               n_heads: int,
               is_cross: bool):
    super().__init__()
    assert emb_dim % n_heads == 0, &quot;Embedding dimension must be divisble by number of heads&quot;
    self.n_heads = n_heads
    self.emb_dim = emb_dim
    self.head_dim = emb_dim // n_heads
    self.is_cross = is_cross
    if not is_cross:
      # This projects each word vector into a new vector space (and we have n_heads amount of different vector spaces)
      self.kqv_project = nn.Linear(self.emb_dim, self.emb_dim * 3, bias=False) 
    else:
      self.kv_project = nn.Linear(self.emb_dim, self.emb_dim * 2, bias=False)
      self.q_project = nn.Linear(self.emb_dim, self.emb_dim, bias=False)
    self.out_project = nn.Linear(self.emb_dim, self.emb_dim)
  
  def forward(self, 
              query: Tensor, # [bsz, q_seq_len, emb_dim]
              kv: Tensor, # [bsz, kv_seq_len, emb_dim]
              # The mask is a tensor where masked positions are -inf and unmasked are 0s
              # This is used for three cases currently: 
              # 1) attention mask for masking future tokens in decoder
              # 2) padding mask for encoder 
              # 3) padding mask for decoder (where we want to mask out tokens that correspond to paddings in kv)
              mask: Optional[Tensor] = None, # [bsz, 1, 1, kv_seq_len] or [bsz, 1, q_seq_len, q_seq_len] where q_seq_len == kv_seq_len for self-attention
              ) -&gt; Tensor:
    bsz, kv_seq_len, _ = kv.shape
    q_seq_len = query.shape[1]

    if not self.is_cross: # self-attention
      assert q_seq_len == kv_seq_len
      kqv = self.kqv_project(kv) # [bsz, kv_seq_len, emb_dim * 3]
      # reshape for multi-head attention
      kqv = kqv.view(bsz, kv_seq_len, self.n_heads, self.head_dim * 3).transpose(1,2) # [bsz, n_heads, seq_len, head_dim * 3]
      K, Q, V = kqv.chunk(3,dim=-1) # K=Q=V =&gt; [bsz, n_heads, seq_len, head_dim]
    else:
      # K, V should come from encoder values, and Q from decoder
      kv_projected = self.kv_project(kv).view(bsz, kv_seq_len, self.n_heads, self.head_dim * 2).transpose(1,2)
      K, V = kv_projected.chunk(2, dim=-1) # [bsz, n_heads, kv_seq_len, head_dim]
      Q = self.q_project(query).view(bsz, -1, self.n_heads, self.head_dim).transpose(1,2) # [bsz, n_heads, q_seq_len, head_dim]

    # print(f&quot;K shape: {K.shape}\t V shape: {V.shape}\t Q shape: {Q.shape}&quot;)
    attn_weights = torch.einsum('bhqd,bhkd-&gt;bhqk',[Q,K]) # [bsz, n_heads, q_seq_len, kv_seq_len] 
    attn_weights /= math.sqrt(self.head_dim) 

    if mask is not None:
      attn_weights += mask

    # softmax across last dim as it represents attention weight for each embedding vector in sequence
    softmax_attn = F.softmax(attn_weights, dim=-1) 
    out = torch.einsum('bhql,bhld-&gt;bhqd',[softmax_attn, V]) # [bsz, n_heads, q_seq_len, head_dim]
    out = out.transpose(1,2).reshape(bsz, -1, self.n_heads * self.head_dim) # [bsz, q_seq_len, emb_dim]
    return self.out_project(out)
</code></pre></details><h3 id=masking-in-the-encoder>Masking in the encoder</h3><p>The last important detail to mention at this point for the encoder is <strong>masking</strong>. Recall that in our input sequence, we used a special token for padding, <code>&lt;PAD></code>. Because these are just dummy tokens added to ensure all sequences in a batch are of the same length, during attention computation we&rsquo;d like to exclude the embedding vectors corresponding to these padding tokens from the weighted sum, by setting their weights to 0.</p><p><img src alt="TODO: Illustration here"></p><p>To do this, we can&rsquo;t just set the weights in the corresponding positions to 0 <em>after</em> softmax, because then the weights will no longer sum to 1. Instead, we can apply a mask to the dot products <strong>before</strong> softmax such that after softmax, their values become 0 &ndash; we do this by adding negative infinity $-\infty$ to positions corresponding to the padding tokens.</p><p>In PyTorch, you can create a padding mask like so:</p><pre><code class=language-python>def create_padding_mask(xs: Tensor, # (B, S)
                        pad_idx: int 
                        ) -&gt; Tensor:
  batch_size, seq_len = xs.shape
  mask = torch.zeros(xs.shape).to(device)
  mask_indices = xs == pad_idx
  mask[mask_indices] = float('-inf')
  return mask.reshape(batch_size,1,1,seq_len) # (B, 1, 1, S)
</code></pre><p>The <code>create_padding_mask()</code> function takes a PyTorch tensor of shape <code>(B,S)</code> and the index of the padding token in vocabulary and returns a mask of shape <code>(B,1,1,S)</code>. As I established earlier in the article, <code>B</code> is the batch size and <code>S</code> is the sequence length. There are 2 additional dimensions in the output because of the way we apply the m ask in MHA:</p><pre><code class=language-python># attn_weights has shape (B, n_heads, query_seq_len, key_value_seq_len)
attn_weights += mask
</code></pre><p>Since <code>mask</code> has shape <code>(B,1,1,S)</code>, we <a href=https://pytorch.org/docs/stable/notes/broadcasting.html target=_blank rel="noreferrer noopener">broadcast</a>
across the 2nd and 3rd dimensions. The 2nd dimension broadcasts across the number of attention heads, while the 3rd dimension broadcasts the number of query vectors. While for self-attention, the number of query vectors equals the number of key and value vectors since they all get generated from the same source vectors, this isn&rsquo;t true in <a href><strong>cross-attention</strong></a>
, which we&rsquo;ll get to later when we look at the decoder. This is why we don&rsquo;t generate a padding mask of shape <code>(B,1,S,S)</code>, although we technically can for self-attention.</p><h3 id=feed-forward-network>Feed-Forward Network</h3><p>Recall that self-attention is only the first part of a Transformer encoder. The issue with only having self-attention is that it is a linear transformation with respect to each element/position in a sequence; as we have seen, self-attention is basically a weighted sum (linear) where the weights are computed from dot products (also linear). And we know that nonlinearities are important in deep learning because it allows neural networks to approximate a wide range of functions (or all continous functions, as the <a href=https://en.wikipedia.org/wiki/Universal_approximation_theorem target=_blank rel="noreferrer noopener">Universal approximation theorem</a>
tells us).</p><p>So the creators of the Transformer introduce a fully connected feed-forward network after attention. This feed-forward network is applied <strong>position-wise</strong>, meaning it is applied to each element in the sequence independently. Therefore, in addition to introducing nonlinearities, these feed-forward networks can also be thought of as somehow &ldquo;processing&rdquo; the individual outputs in the sequence post-attention &ndash; it does this by projecting the input into a higher dimension, applying nonlinearity, and projecting it back into the original dimension.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> In the paper, they use a 2-layer network with 1 hidden layer and ReLU activation as the nonlinearity. In PyTorch, this simply implemented as:</p><pre><code class=language-python>feed_foward_net = nn.Sequential(
    nn.Linear(embedding_dimension, hidden_dimension),
    nn.ReLU(),
    nn.Linear(hidden_dimension, embedding_dimension),
) 
</code></pre><p>In the paper, <code>hidden_dimension</code> is set to a value of 2048 (embedding dimension is 512 as mentioned earlier).</p><p>To recap: if we have a tensor of shape <code>(9,512)</code> at the start of the encoder, after passing through Multi-Head Attention, we get back an output of the same shape. When we pass this through the feed forward net as defined above, we basically pass each of the nine <code>512</code>-dimensional vectors through the neural network in parallel and join them together to get back a final output tensor of the same shape <code>(9,512)</code>. This works because the last dimension of the input tensor (512) is the same as the dimension of the input features of the network. Note that again, I excluded the batch dimension (i.e. in a mini-batch, the tensors will be of shape <code>(B,T,D)</code> instead of <code>(T,D)</code> that I have used in this example) because the same analysis holds even for bigger batch sizes.</p><h3 id=encoder-the-remaining-bits>Encoder: the remaining bits</h3><p>Here are the remaining details for the encoder:</p><ul><li><a href=https://arxiv.org/abs/1607.06450 target=_blank rel="noreferrer noopener">Layer normalization</a>
is applied to the output of each sublayer. Personally, I was confused by this initially because some illustrations of how layer norm works uses layer norm in the context of Computer Vision and Convolutional Neural Networks, which is slightly different from how it is used in Transformers (be careful, some explanations online confuse between the two as well). For this, I&rsquo;ve found the following figure from the paper <a href=https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf target=_blank rel="noreferrer noopener">&ldquo;Leveraging Batch Normalization for Vision Transformers&rdquo; (Yao, Zhuliang, et al., 2021)</a>
to be helpful in visualising the key difference between layer norm in CNN and in transformers:</li></ul><p align=center width=100%><img src=https://sarckk.github.io/media/layernorm.png></p>- Use of **residual connections** around both the self-attention and feed-forward network sublayers. First introduced in 2015 by the famous [ResNet paper](https://arxiv.org/abs/1512.03385), residual connections here basically means instead of the sublayer output being `f(x)`, it is `x + f(x)`, which helps with training by providing a gateway for gradients to pass through more easily during backprop.<hr><p>Let&rsquo;s end this section by revisiting the encoder diagram from the paper:</p><p align=center width=100%><img src=https://sarckk.github.io/media/encoder_here.png></p><p>Everything shown in the diagram should be familiar to us by now. In particular, note how there are 3 arrows going into the Multi-Head Attention module: these represent key, query and values.</p><hr><h2 id=decoder>Decoder</h2><p>Phew! There was quite a lot to cover for encoders. Fortunately, I&rsquo;ve already covered most of the important parts of the Transformer &ndash; the decoding part more or less mirrors what we had in the encoding phase, with a few key differences.</p><p>The input to the first decoder in the stack is a sequence of numerical representations of output tokens. The term &ldquo;output&rdquo; here might be a bit confusing. In the context of neural machine translation, the output here refers to tokens in the target language. Assuming that the target language is German and that we use a word-level tokenizer (i.e. each token is just a German word), then we can say that we pass in the sequence of indices of each German word in the sentence. The rest is the same as encoders: we generate an embedding vector and add positional encoding.</p><p>Also similar to the encoding phase, we have $N$ decoder modules, where $N=6$ for the base model in the original paper. Each decoder is similar to the encoder, except there are 2 differences:</p><ul><li>In a decoder, there is an additional sublayer between self-attention and feed-forward network: <strong>cross-attention</strong>.</li><li>An additional mask is used in the decoder to prevent &ldquo;looking into the future&rdquo; in self-attention.</li></ul><h3 id=difference-1-cross-attention>Difference #1: Cross-Attention</h3><p>Remember that in self-attention, we have query, key and value vectors used in attention computation coming from the same embedding vector. In cross-attention, we derive the query vector from one embedding vector and key and value vectors from a different vector. More specifically, in the decoder, the query vector comee from the output of the previous layer (i.e. for the very first decoder, this is the embedding layer; for subsequent decoders, it&rsquo;s the previous decoder), while the key and value vectors are generated from the output of the last encoder. Referring back to <a href>Figure 1</a>
from the paper, this is illustrated with two arrows coming from the encoder to the cross-attention sublayer of the decoder.</p><p>Again, in the context of the machine translation task that the original Transformer was trained on, this step makes intuitive sense because for each word (or more generally, a token) in the target language sentence, we are essentially querying for the most relevant word(s) in the source language sentence and taking a weighted sum of their vector representations so that in the end we can predict the next word (more on training soon):</p><p align=center style=display:flex;flex-direction:column;align-items:center><img src width=400><caption>Illustration of the concept here</caption></p><h3 id=difference-2-masking-in-self-attention>Difference #2: Masking in self-attention</h3><p>The other difference between decoders and encoders is in the masking. Within the decoder, the masks used in self-attention and cross-attention are different, too.</p><p><strong>Masking in self-attention</strong></p><p>First, let&rsquo;s talk about masking in self-attention. In the decoding stage, the objective is: for each position, correctly predict what token should be at the next position. Therefore, unlike in the encoder, we can&rsquo;t access information about future positions to make a prediction for any given position in the sequence. This is especially true at inference time, where we do not have access to what comes after the current token by definition (otherwise we wouldn&rsquo;t need the model at all), but during training time, we do have information about the full translated sequence so we need to mask out, for each position, all the positions that come after it. It&rsquo;s best to illustrate this with a figure:</p><p align=center style=display:flex;flex-direction:column;align-items:center><img src width=400><caption>Illustration of the concept here</caption></p><p>This is a mask where the upper triangular part are negative infinities. I don&rsquo;t think there&rsquo;s an official name for this mask, so I&rsquo;ll call it the <strong>look-ahead</strong> mask. Here&rsquo;s the code that generates this mask:</p><pre><code class=language-python>def create_decoder_mask(seq_len: int) -&gt; Tensor:
  mask = torch.zeros(seq_len, seq_len).to(device)
  mask_indices = torch.arange(seq_len)[None, :] &gt; torch.arange(seq_len)[:, None] 
  mask[mask_indices] = float('-inf')  
  return mask.reshape(1,1,seq_len,seq_len) # (1, 1, S, S)
</code></pre><p>The function takes in the length of the target sequence and returns a mask with shape <code>(1,1,S,S)</code>. Recall that the self-attention mask for the <strong>encoder</strong> had shape <code>(B,1,1,S)</code>. In the decoder, I&rsquo;ve set the first dimension as <code>1</code> for broadcasting, but it can very well be <code>B</code> as well. However, the third dimension has to be <code>S</code> and not <code>1</code>, since the mask used in the decoder is two-dimensional, and thus needed to be $S \times S$.</p><p>When I was researching on how masking works in the decoder, the examples I could find also added a mask to exclude the padding tokens, as we did in the encoder. However, I don&rsquo;t think this is actually needed for self-attention in the decoder. Here&rsquo;s a brain dump of my reasoning:</p><blockquote><p>For any given position $i$, There are two possibilities:</p><ol><li>It <em>isn&rsquo;t</em> a padding token. Due to our look-ahead mask, we don&rsquo;t consider any tokens that comes afterwards in the weighted sum. Any tokens before it are necessarily not padding tokens because position $i$ isn&rsquo;t a padding token and we can&rsquo;t have a padding token that comes before a non-padding token (except the final <code>&lt;EOS></code> token, but this isn&rsquo;t included in the input to the decoder during training because the decoder inputs are shifted right &ndash; more on this in the upcoming section on <a href>Training</a>
).</li><li>It <em>is</em> a padding token. Again, we don&rsquo;t consider any tokens that comes afterwards. The positions before it might have padding tokens. So the output of attention at position $i$ would wrongly have included information from some padding tokens, but this doesn&rsquo;t matter because in the final loss calculation we ignore all positions with padding tokens (again, more on this later).</li></ol></blockquote><p>I haven&rsquo;t found a resource online that explicitly confirms this, so I could very well wrong &ndash; if so, please let me know by submitting an issue <a href=https://github.com/sarckk/sarckk.github.io target=_blank rel="noreferrer noopener">here</a>
.</p><p><strong>Masking in cross-attention</strong></p><p>In cross-attention, because the key and value vectors that we&rsquo;re using to calculate dot products and calculate the weighted sum respectively come from the final output of the encoder, we don&rsquo;t need to mask future tokens, because all of this information should be available to us in the decoding stage. However, we still need to mask out the positions which correspond to padding tokens in the encoder&rsquo;s input like we did for self-attention in the encoder.</p><p>Recall the <a href>shape of the padding mask</a>
, which was <code>(B,1,1,S)</code>, where <code>S</code> is the length of the source token sequence. Let&rsquo;s think about the shape of the dot product matrix, $QK^T$. Let <code>T</code> be the target token sequence length. Then, $QK^T$ will be a matrix $\in \R^{T \times S}$. When we consider multiple heads and batch size, then the result of our attention weights will be of shape <code>(B,n_heads,T,S)</code>. Using broadcasting, we just add the padding mask to these attention weights, and calculate the weighted sum.</p><p>That&rsquo;s it!</p><hr><h2 id=linear--softmax-layer>Linear + Softmax Layer</h2><p>As a final step, the output from the last decoder is passed to a linear layer that projects the embedding vectors to a dimension given by the vocabulary size of the target language, followed by a softmax layer to convert those values into probabilities that sum to 1. For example, if we translating from English to German, and the dataset we&rsquo;re training on has a German vocabulary size of 37000, then the linear layer will take <code>emb_dimension</code> input features (e.g. 512) and have 37000 output features. After softmax, the value at $i^{th}$ dimension of the vector at $k^{th}$ position in the sequence will be probability for the $i^{th}$ word/token in the vocabulary in the $(k+1)^{th}$ position.</p><h1 id=training>Training</h1><p>Now let&rsquo;s talk about training. You can train Transformers on any sequence to sequence tasks but I&rsquo;ll talk about it in the context of machine translation, since this is what the original Transformer was trained for.</p><p>In a typical training dataset, like the WMT 2014 English-German dataset, you&rsquo;ll have pairs of (sentence in source langauge, same sentence in target language). As with most NLP tasks, first you&rsquo;ll use a tokenizer to split the sentences into their tokens, build up vocabulary of these tokens, and map the sequence of tokens into their corresponding indices in this vocabulary. Then we embed the tokens using the process <a href>explained earlier</a>
. This is all pretty standard stuff &ndash; the thing I want to highlight is that the input to the decoder (i.e. target sequence) is shifted one to the right, which just means you exclude the last token. In code, you&rsquo;d do something like:</p><pre><code class=language-python>target_input = target[:,:-1]
</code></pre><p>To give a concrete example:</p><ul><li>The source sequence is <code>[&lt;BOS>, I, am, feeling, great, today, &lt;EOS>]</code></li><li>The translated target sequence in German is <code>[&lt;BOS>, Ich, f√ºhle, mich, heute, gro√üartig, &lt;EOS>]</code></li><li>The input to the encoder is the embedding of <code>[&lt;BOS>, I, am, feeling, great, today, &lt;EOS>]</code></li><li>The input to the decoder is the embedding of <code>[&lt;BOS>, I, am, feeling, great, today]</code> (shifted right)</li><li>The decoder should predict <code>[I, am, feeling, great, today, &lt;EOS>]</code> (shifted left).</li></ul><p>So the decoder learns to predict, at each position, the token that appears in the next position.</p><h3 id=loss-function>Loss function</h3><p>The paper doesn&rsquo;t explicilty mention what loss function is used, but you should be able to use any multi-class classification loss (which is what we&rsquo;re doing when predicting the most probable next token). The implementations I&rsquo;ve seen seem to use either cross-entropy or KL divergence loss. In my own implementation, I&rsquo;ve used cross-entropy loss.</p><p>A mistake that cost me a lot of time debugging was how <code>nn.CrossEntropyLoss</code> works in PyTorch. In PyTorch, this module <strong>performs softmax</strong> before calculating the actual cross entropy loss &ndash; it should really be named something like <code>nn.SoftmaxCrossEntropyLoss</code>! Because the figure of the Transformer architecture in the original paper has a softmax layer, this is what I originally implemented, and I was passing these normalized logits directly to <code>nn.CrossEntropyLoss</code>, causing issues during training: loss plateauing and my model quickly converging to producing the same tokens. In fairness, the PyTorch docs does mention that it expects an input that <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html target=_blank rel="noreferrer noopener">&ldquo;contains the unnormalized logits for each class (which do not need to be positive or sum to 1, in general)&rdquo;</a>
but who has time to read documentation, am I right? üòè</p><p>When calculating the loss, it&rsquo;s important to <strong>ignore the loss contributed by the positions that correspond to the padding tokens</strong>. This is because the padding masks themselves are not sufficient to completely get rid of the influence of padding tokens on our loss. For example, consider the following attention matrix and padding mask:</p><p><img src alt></p><p>The padding mask serves to prevent the embeddings of padding tokens from being included in the weighted sum in attention, but we still compute the weighted sum for the padding positions (in the above figure, that would be the bottom 2 rows/positions). If this was the final output of the last decoder, after passing through the linear layer and thereafter taking softmax, we would have the next-token probabilities at each position, even where we had paddings! So we&rsquo;d like to exclude these positions from our loss. In <code>nn.CrossEntropyLoss</code>, you can do this by passing the index of <code>&lt;PAD></code> to the <code>ignore_index</code> argument.</p><h3 id=regularization>Regularization</h3><p>Section <strong>5.4</strong> of the paper mentions two regularization techniques used during training:</p><ol><li><strong>Label smoothing</strong> of 10% is also used in the loss calculation. The idea is simple: instead of the target distribution being one-hot (i.e. the &ldquo;target&rdquo; word has probability 1 and the rest of the words have 0), we set the probability of one word to be 0.9 and then distribute the other 0.1 over rest of the words in the vocabulary. This gives the model more flexibility in what token it predicts and presumably improves training. Intuitively, this kind of smoothing makes sense because with languages, there are often many plausible words that can come after some sequence of them.</li><li><a href=https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/ target=_blank rel="noreferrer noopener"><strong>Dropout</strong></a>
is used after each sublayer in the encoders, decoders, as well as to all the embeddings. They use a 10% dropout for the base model.</li></ol><h3 id=model-hyperparameters>Model hyperparameters</h3><p>Model hyperparameters for the base Transformer model can be found on Table 3 of the paper. This model has around 65M trainable parameters &ndash; comparing this number to the number of trainable params in your own implementation can be a good sanity check during development.</p><h3 id=optimizer>Optimizer</h3><p>Nothing special here, just an Adam optimizer with $\beta_1=0.9$, $\beta_2=0.98$ and $\epsilon=10^{-9}$.</p><h3 id=learning-rate-scheduling>Learning-rate scheduling</h3><p>The paper uses a variable learning rate during training, given by the following formula:</p><p>\begin{equation}
lrate = {d_{model}}^{-0.5} \cdot min(step_num^{-0.5}, step_num \cdot warmup_steps^{-1.5})
\end{equation}</p><p>where $warmup_steps=4000$.</p><p>The best way to understand how this works is to look at how the learning rate changes with step count:</p><p align=center style=display:flex;flex-direction:column;align-items:center><img src=https://raw.githubusercontent.com/gordicaleksa/pytorch-original-transformer/main/data/readme_pics/custom_learning_rate_schedule.PNG width=500/>
<span style=display:flex;flex-direction:row><span>Source: </span><a href=https://github.com/gordicaleksa/pytorch-original-transformer>https://github.com/gordicaleksa/pytorch-original-transformer</a></span></p><p>If you got the model and training steps right, you should be seeing a sweet training loss curve like this:</p><p align=center><img src=https://sarckk.github.io/media/train_loss_transformer.svg width=500/></p><h1 id=inference>Inference</h1><p>Now, let&rsquo;s talk about how the Transformer works at inference time for machine translation. Transformers are <strong>auto-regressive</strong> models, meaning that it predicts the next token from all the previous tokens. At inference time, the only data available to us is the sentence(s) in the source language. From this, how do we generate the translated text using our trained model?</p><p>This is achieved by starting with the <code>&lt;BOS></code> token, to mark the beginning of the translated sentence. Then, follow these steps:</p><ol><li>First, embed the entire source sequence, pass this through the encoders and obtain the final output, which is a latent representation of the source sentence that the trained model has learned.</li><li>Embed the <code>&lt;BOS></code> token, pass this through the decoders &ndash; which will use the output from the encoders in cross-attention &ndash; and after linear+softmax layers we get a probability distribution (over the vocabulary of the target language) for what the next token should be.</li><li>We can greedily choose the token with the highest probability, and append this token to <code>&lt;BOS></code>, getting us a longer target sequence.</li></ol><p>All we have to do now is to simply repeat steps <strong>2</strong> and <strong>3</strong> until the <strong>last predicted token is <code>&lt;EOS></code>, marking the end of sentence</strong>. Viola! We just translated from one language to another.</p><p>Note that we still have to use the padding and the look-ahead masks just like we did for training, and in step 2 we would have to use a different look-ahead mask with each iteration since the target sequence length changes. However, more advanced implementations of Transformer can allow <a href=https://datascience.stackexchange.com/questions/80826/transformer-masking-during-training-or-inference target=_blank rel="noreferrer noopener">only the last predicted token to be passed in on each iteration</a>
, in which case the look-ahead mask doesn&rsquo;t have to be passed in, since there is no &ldquo;future&rdquo; to consider.</p><h1 id=conclusion>Conclusion</h1><p>As I mentioned in the beginning of this article, I wrote this post mostly for myself, as a sort of recap of what I&rsquo;ve learned. That said, even with so many tutorials online on this now-6-years-old technology, I still think what I have here might be useful to anyone who might across it, especially if they are somewhat new to deep learning; most tutorials I&rsquo;ve seen on Transformers tend to gloss over some details especially around masking as well as how the encoder/decoder input vectors, and the key,query and value vectors in attention are generated. What I&rsquo;ve attempted to do in this post is to document everything that I&rsquo;ve had to learn and understand to train a simple Transformer model. I&rsquo;ve already learned a lot writing this article, but I hope that it also helps someone out there.</p><hr><h1 id=acknowledgements>Acknowledgements</h1><p>Here are some resources that I&rsquo;ve used to learn about Transformers</p><ul><li><a href="https://www.youtube.com/watch?v=ptuGllU5SQQ" target=_blank rel="noreferrer noopener">Stanford CS224N Lecture 9 - Self-Attention and Transformers</a>
. Probably the best lecture on Transformers online. This is the video that made attention and Transformers <em>click</em> for me.</li><li><a href=https://github.com/gordicaleksa/pytorch-original-transformer target=_blank rel="noreferrer noopener">PyTorch implementation of Transformer by Gordic Aleksa (AI Epiphany)</a>
and the accompanying <a href="https://www.youtube.com/watch?v=n9sLZPLOxG8" target=_blank rel="noreferrer noopener">video tutorial</a>
. I didn&rsquo;t refer to the implementation for the most part, but I did use it as reference to debug an issue I had with my implementation.</li></ul><h1 id=where-to-go-from-here>Where to go from here</h1><p>As an addendum, I&rsquo;ll just include some links to resources that I&rsquo;ve come across that might serve as good next steps after understanding the Transformer. These are also things I&rsquo;d like to read up in the near future:</p><ul><li><a href=https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/ target=_blank rel="noreferrer noopener">A review of Transformer family of models</a></li><li><a href=https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J target=_blank rel="noreferrer noopener">Mechanistic Interpretability research</a><ul><li>Related: <a href=https://transformer-circuits.pub/2021/framework/index.html target=_blank rel="noreferrer noopener">A Mathematical Framework for Transformer Circuits</a></li></ul></li><li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target=_blank rel="noreferrer noopener">Kaparthy&rsquo;s GPT from scratch</a></li><li><a href=https://jaykmody.com/blog/gpt-from-scratch/ target=_blank rel="noreferrer noopener">GPT in 160 lines of numpy</a><ul><li>Related: check out my previous blog post on <a href=https://sarckk.github.io/post/2022/03/20/convolutional-neural-networks-from-scratch/ target=_blank rel="noreferrer noopener">CNNs from scratch in numpy</a>
.</li></ul></li><li><a href=https://arxiv.org/abs/1810.04805 target=_blank rel="noreferrer noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href=https://arxiv.org/abs/2006.04768 target=_blank rel="noreferrer noopener">Linformer: Self-Attention with Linear Complexity</a></li><li><a href=https://arxiv.org/abs/2205.14135 target=_blank rel="noreferrer noopener">Flash Attention</a></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>It looks like the exact role that these feed-forward networks play in a transformer is not fully understood; see <a href=https://aclanthology.org/2021.emnlp-main.446.pdf target=_blank rel="noreferrer noopener">&ldquo;Transformer feed-forward layers are key-value memories.&rdquo; (Geva, Mor, et al., 2020)</a>
for a paper that tries to shed light into their function.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></main><footer><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
</script><script defer src=//yihui.org/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//yihui.org/js/center-img.js></script><hr><a href=https://github.com/sarckk target=_blank rel="noreferrer noopener">Github</a></footer></body></html>