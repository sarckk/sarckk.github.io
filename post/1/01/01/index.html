<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>| Shin</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/base16/gruvbox-dark-medium.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/highlight.min.js></script>
<script>hljs.highlightAll()</script></head><body><a href=/ style=all:initial;cursor:pointer><h1>Bits&Brains</h1></a><hr><div class=article-meta><h1><span class=title></span></h1><p class=terms></p></div><main><p>p&mdash;
title: Grokking Transformers (WIP)
date: &lsquo;2023-04-10&rsquo;
categories:</p><ul><li>AI
tags:</li><li>AI</li><li>LLM</li></ul><hr><p>Today&rsquo;s Large Language Models (LLM) are based on Transformers, a deep learning model architecture for sequence-to-sequence transformations based on the attention mechanism. While it was originally proposed and used in Natural Language Processing (NLP) tasks like language translation, it turns out that a lot of things that we care about can be modelled in terms of sequences, making transformers a useful model in a wide variety of applications beyond NLP, such as <a href>image processing</a>
and <a href>reinforcement learning</a>
. Given the overwhelming success of transformers in deep learning and the outsized impact that transformer-based generative AI (e.g. GPT) has had &ndash; and will likely continue to have &ndash; on our society, I thought I should finally take time to read and understand the paper <a href=https://arxiv.org/abs/1706.03762 target=_blank rel="noreferrer noopener">&ldquo;Attention Is All You Need&rdquo; (Vaswani et al., 2017)</a>
that first proposed Transformers. That paper is now almost 6 years old(!) but better late than never, right?</p><p>There are already many writings covering transformers on the internet, so this article is mostly for my own learning &ndash; this is already <a href=https://ideas.time.com/2011/11/30/the-protege-effect/ target=_blank rel="noreferrer noopener">well documented</a>
, but I find that writing in a pedagogical style helps immensely in solidifying in my learnings and is almost always worth the effort. If anyone else stumbles across this post and finds it helpful, that&rsquo;s an added bonus!</p><p>This post will cover the technical details behind the Transformer model. The core concept behind transformers &ndash; self- and cross- attention &ndash; really isn&rsquo;t too hard too grasp, but I&rsquo;ve found that actually getting your hands dirty and implementing the model in Pytorch elevates your understanding of the material. Personally, I ran into many issues while trying to write and train the model that I wouldn&rsquo;t have known had I stopped at reading the paper or other tutorials online.</p><h1 id=all-about-transformations>All about transformations</h1><p>A Transformer &ndash; as its name suggests &ndash; <em>transforms</em> an input sequence $(x_1,x_2,&mldr;,x_n)$ into an output sequence $(y_1,y_2,&mldr;,y_m)$. Because this formulation is so general, it doesn&rsquo;t say what $x_1$ and $y_1$ should represent &ndash; it could be a word, a sub-word, a character, a pixel, or a token representing any arbitrary thing. However, I&rsquo;ll be talking about Transformers in the context of NLP here, because that&rsquo;s what it was originally invented for. So if we&rsquo;re talking about machine translation, the input sequence could be a sequence of words in one language (e.g. Korean) and the output could be a sequence of words in the target language (e.g. English):</p><p><img src=https://sarckk.github.io/media/transformer_1.svg alt></p><p>In the diagram above, each element in a sequence represents a word for simplicity, but in practice, it is common for this to a smaller unit than a word, like a subword for example. This depends on the tokenizer you use.</p><h1 id=diving-into-transformers>Diving into Transformers</h1><p>Now let&rsquo;s talk about what a Transformer actually looks like. From the original paper:</p><p align=center width=100%><img src=https://sarckk.github.io/media/transformer_architecture.png width=450/></p><p>If you&rsquo;re like me, this might be a bit overwhelming to take in at first. In reality, there&rsquo;s only 4 major components to a transformer architecture: the embedding layer, the encoder, the decoder, and the final linear+softmax layers that transform the output of the decoder into probabilities. Here&rsquo;s the same diagram with some annotations overlaid on top:</p><p align=center width=100%><img src=https://sarckk.github.io/media/transformer_arch_illustrated.png width=400/></p><p>At a high level, here&rsquo;s the journey that our input sequence takes to be transformed into an output sequence:</p><ol><li>Go through input embedding layer which projects each element in a sequence $x_i$ into a higher dimensional vector.</li><li>Add &ldquo;Positional Encoding&rdquo; vector to each element in the sequence (which remember, is now a high-dimensional vector). We&rsquo;ll talk about this in more detail later.</li><li>Go through the Encoder (orange block in the diagram above) <strong>N</strong> times. These <strong>N</strong> encoders have the same architecture but do not share weights.At the end of this step, we get a tensor that compactly represents the input sequence.</li><li>On the decoder side, we pass in a sequence of length <strong>M</strong>, which goes through the same embedding layer + positional encoding as we had for the input sequence.</li><li>The output embedding goes through a stack of <strong>N</strong> decoders (again no sharing of weights), each of which uses the tensor we got from <strong>Step 3</strong> in some way. At the end of this step, we get another tensor.</li><li>We pass this tensor through a final Linear + Softmax layer to obtain <strong>M</strong> probabilities, where again <strong>M</strong> is the length of the sequence we passed into the decoder.</li><li>We convert those probabilities into actual tokens (e.g. by taking the token with the highest probability), giving us an output sequence of length <strong>M</strong>.</li></ol><p>So from an input sequence of length <strong>N</strong> and decoder input of length <strong>M</strong>, we got &ndash; as the final product of the Transformer &ndash; another sequence of length <strong>M</strong>.</p><p>It&rsquo;s okay if some of these steps do not make sense yet, especially on why we need a sequence of length <strong>M</strong> to pass into the decoder to get another sequence of the same length as the output. I was initially confused by this as well: if we are just passing in an input sentence like "" and expect the model to output the translated text, what are we passing into the decoder? WTF? Don&rsquo;t worry, this will become clear when we talk about Transformers at training and inference time, later in this article.</p><p>Now, let&rsquo;s talk about each of these components in greater detail during <strong>training</strong>. Then we&rsquo;ll talk about what happens at <strong>inference time</strong>.</p><h1 id=diving-deeper-the-embedding-layer>Diving deeper: the embedding layer</h1><p>Let&rsquo;s start from the very beginning. The original Transformer in the 2017 paper was trained on the task of translating English sentences to German, using the WMT 2014 English-German dataset. This dataset contains ~4.5 million pairs of English sentence and its corresponding translation in German. We&rsquo;ll use this example to explain what happens in a Transformer for the rest of the article.</p><p>This means that during training, the input to the transformer (bottom left of Figure 1 above, below the very first decoder) is a sequence(s) of tokens in the English sentence. The paper mentions that they used <a href=https://en.wikipedia.org/wiki/Byte_pair_encoding target=_blank rel="noreferrer noopener">byte-pair</a>
encoding (page 7) to tokenize the sentences, but for simplicity I&rsquo;ll assume that the sentences are tokenized word by word:</p><p><img src=https://sarckk.github.io/media/seq_len_input.svg alt></p><p>Above is an illustration of what this would look like for a mini-batch of 3 English sentences. Note that we have 3 special tokens: <code>&lt;BOS></code> denoting the beginning of sentence, <code>&lt;EOS></code> marking the end of a sentence, and <code>&lt;PAD></code> representing an &ldquo;empty&rdquo; token to make all the sequences in the tensor of the same length (i.e. the maximum sequence length across all sentences in the mini-batch).</p><h2 id=step-1-string---number>Step 1: String -> Number</h2><p>Before we can pass this into the encoder, we need to convert these sequences of strings into a numerical representation instead so we can do some computation and make GPUs go <em>brrr</em>. To do this, we create &ndash; from the dataset &ndash; a mapping from all possible tokens (in this case, words) to its index in a vocabulary. So, the word &ldquo;The&rdquo; might have an index of 37, which uniquely identifies that token. We also leave out some indices for the special tokens that we introduced (<code>&lt;BOS></code>, <code>&lt;EOS></code> and <code>&lt;PAD></code>): for example, their indices might be 0, 1 and 2 respectively.</p><h2 id=step-2-number---embedding-vector>Step 2: Number -> Embedding Vector</h2><p>Once we have the input tensor, we turn each word (which by this point is a number representing its index in our vocabulary) into a high-dimensional vector that we call an <strong>embedding vector</strong>. In the original paper, the embedding vector is <strong>512-dimensional</strong>, but this is a hyperparameter that we can tune for our model through experiments.</p><p>After this step, we have a tensor of shape (batch size, (max) input sequence length, embedding dimension). For the rest of the article, I&rsquo;ll use the short form <code>B</code> for batch size, <code>T</code> for the maximum input sequence length, and <code>D</code> for the embedding dimension. In our example, <code>B=3</code>, <code>T=9</code> and <code>D=512</code>.</p><h2 id=step-3-adding-positional-encoding>Step 3: Adding Positional Encoding</h2><p>We&rsquo;ll actually skip this step for now, because it only really makes sense when we start looking at the Encoder and self-attention. Without too much detail, though, here we basically add a tensor with the same shape <code>(B,T,D)</code> to our tensor from <strong>Step 2</strong>. This tensor that we add encodes information about the relative order of each word in a sentence, since this is information that we&rsquo;d like the Transformer to consider in the computation of the final output probabilities. We&rsquo;ll come back to this shortly.</p><p>In the end, the input to the Encoder is a tensor of shape <code>(B,T,D)</code>. The same thing happens for the Decoder, except with German sentences in our example of English-to-German translation.</p><h1 id=encoder>Encoder</h1><p>We&rsquo;ve finally reached the Encoder. The encoding step consists of <strong>N</strong> independent Encoder units stacked on top of each other. These Encoders are identical in architecture, but do not share weights between them and are thus separately updated during backpropagation.</p><p>The Encoder unit itself comprises 2 parts:</p><ol><li>A Self-Attention module, followed by</li><li>A Feed Forward network.</li></ol><p>We&rsquo;ll start with this high level picture of the Encoder and gradually fill in more details:</p><p align=center width=100%><img src=https://sarckk.github.io/media/encoder_here.png width=300/></p><h2 id=attention-is-all-you-need>Attention is all you need</h2><p>The <strong>core idea</strong> behind the Transformer is to replace recurrence and convolutions that made up previous sequence-to-sequence models with one entirely based on the attention mechanism. In simple terms, the attention mechanism is basically just taking a bunch of dot products between sequences. And <strong>self-attention</strong> is just particular case of attention where the sequences that we&rsquo;re concerned with is actually all the same &ndash; just one sequence.</p><p>Remember that at this point, our example input to the Encoder is a tensor of shape <code>(3,9,512)</code>. To make the explanation easier, let&rsquo;s look at what happens for <strong>one</strong> single sentence out of 3 total sentences in this mini-batch: when we actually pass through the entire batch with 3 sentences, logically it will be as if we pass through each sentence separately and merge the 3 outputs together.</p><p>Let&rsquo;s look at just one sentence: &ldquo;This jacket is too small for me&rdquo;. After the embedding layer, we have a tensor of shape <code>(9,512)</code>. To encode this tensor, we essentially pass all 9 of the 512-dimensional embedding vector to the self-attention module <strong>at the same time</strong>:</p><p><img src alt></p><p>The goal of the self-attention module is to figure out how the words in the sentence (or more generally, tokens in a sequence) relate to each other.</p><p>Remember, the sentence is &ldquo;This jacket is too small for me&rdquo;. When we look at the adjective <code>"small"</code>, we want to understand what object it is referring to. Clearly, we know that it is referring to the <code>"jacket"</code>, but the Transformer model has to learn this. In other words, it has to learn how each word relates to another. In vector space, we have a concept for computing the similarity between vectors: <strong>dot product</strong>.</p><p>Dot products form the basis of the attention mechanism.</p><h2 id=the-attention-mechanism>The Attention mechanism</h2><p>The attention mechanism involves 3 components: query(s), keys(s), and value(s) where these are all vectors. More formally, we have:</p><ul><li>Queries $q_1,&mldr;,q_T$ where $q_i \in$ $\R^{d_k}$, where $d_k$ is the dimension of the query vector, and $T$ is the number of queries</li><li>Keys $k_1,&mldr;k_K$ where $k_i \in$ $\R^{d_k}$, and $K$ is the number of key-value pairs</li><li>Value $v_1,&mldr;,v_K$ where $v_i \in$ $\R^{d_v}$, where $d_v$ is the dimension of the value vector, which is not necessarily equal to $Q$, although in our case of English-German translation, it is.</li></ul><p>Note that $T$, the number of queries doesn&rsquo;t necessarily have to equal $K$, the number of key-value pairs, but the number of keys must be the same as the number of values (for them to form a key-value pair). Furthermore, the query and key vectors must have the same dimension, $d_k$ so we can do a dot product.</p><p>Given this formulation, the attention function on $q_i$ does the following:</p><ul><li>Get dot product of query and key vectors to get a scalar value: $\alpha_{ij} = q_i \cdot k_i$</li><li>Normalize each dot product $\alpha_{ij}$ by performing <a href=https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax target=_blank rel="noreferrer noopener">softmax</a>
across all $j$, where $j=0,&mldr;,K$ , where $K$ is the number of key-value pairs. This gives us weights $w_{ij}$ for all $j$.</li><li>Output weighted sum of all $v_j$ where weight of $v_j$ is $w_{ij}$.</li></ul><h2 id=back-to-our-example>Back to our example</h2><p>Back to our example sentence, we have $x_1,&mldr;,x_9$ where $x_i$ is a 512-dimensional embedding vector representing each word in the sentence <code>"This jacket is too small for me"</code> plus the <code>&lt;BOS></code> and <code>&lt;EOS></code> tokens. We obtain our query, key and value vectors from $x_i$ by multiplying it each time with a different matrix:</p><p>$$\begin{aligned}
k_i = Kx_i, \text{where } K \text{ is a } d_k \times d_k \text{matrix} \
q_i = Qx_i, \text{where } Q \text{ is a } d_k \times d_k \text{matrix} \
v_i = Vx_i, \text{where } V \text{ is a } d_v \times d_v \text{matrix}
\end{aligned}$$</p><p>In our case, $d_k=d_v=512$. We have $K$, $Q$ and $V$ matrices (and therefore independently traininable) that linearly project each key, query and value vectors &ndash; this allows for more flexibility in both how the model chooses to define &ldquo;similarity&rdquo; between words (by updating $K$ and $Q$), as well as what the final weighted sum represents (by updating $V$) in latent space. In Pytorch code, these matrices are implemented as <a href=https://pytorch.org/docs/stable/generated/torch.nn.Linear.html target=_blank rel="noreferrer noopener"><code>nn.Linear()</code></a>
modules with <code>bias=False</code>.</p><p>Now that we have $k_i$, $q_i$ and $v_i$, we just compute the corresponding output for $x_i$ using the steps outlined earlier, computing the sum of all vectors weighed by the dot products. Here, since $q_i$,$k_i$ and $v_i$ are all derived from $x_i$, we give it a special name: <strong>self-attention</strong>.</p><p><img src alt="Illustration of self-attention"></p><h2 id=parallelizing-attention-computation>Parallelizing attention computation</h2><p>As you can see, attention is computed using dot products between any two words within a sequence, allowing the Transformer to learn long-range dependencies in a sequence more easily. One downside of this, though, is that the computation of attention scores is quadratic in the length of the input sequence $N$. This quadratic $O(N^2)$ complexity is an issue because it means it will take a lot of compute for long sequences.</p><p>Fortunately, we can represent the computation as a product of a few matrix multiplications, which is easily parallelizable on GPU/TPUs. Remember, our input tensor has shape <code>(9,512)</code> &ndash; this is essentially a matrix of dimension 9 by 512. Given this matrix $X$ and the aforementioned linear projection matrices $K$, $Q$ and $V$, we can formulate the attention computation as follows:</p><p>\begin{equation}
Attention(X, Q,K,V) = softmax(\frac{(XQ)(XK)^T})XV
\end{equation}</p><p>The authors in the Transformers paper also apply a scaling factor of $\frac{1}{d_k}$ to the matrix of dot products (numerator) to prevent the products from becoming too large, which can &ldquo;[push] the softmax function into regions where it has extremely small gradients&rdquo; (<a href=https://arxiv.org/pdf/1706.03762.pdf target=_blank rel="noreferrer noopener">Viswani et al, 2017, pg 4</a>
):</p><p>\begin{equation}
Attention(X,Q,K,V) = softmax(\frac{(XQ)(XK)^T}{\sqrt{d_k}})XV
\end{equation}</p><p>This ability for parallelization is a part of why the Transformer has been so successful &ndash; previous models based on recurrence, for example, cannot be parallelized because the computation of its state at time $t$, $h_t$ necessarily depends on the computation of its previous state at time $t-1$, $h_{t-1}$.</p><h2 id=multi-head-attention>Multi-Head Attention</h2><p>In the paper, the authors use <strong>Multi-Head Attention (MHA)</strong>. In MHA we have multiple &ldquo;heads&rdquo; that each performs the attention computation that we <em>just</em> talked about. However, each head $h_i$ has its own linear projection matrices $K_i$, $Q_i$, and $V_i$, and these matrices project the key, query and value vectors to a <strong>lower</strong> dimensional space than we had with single matrices.</p><p>For example, if the dimension of matrix $K$ in <strong>Single-Head Attention</strong> was $512 \times 512$, then the dimension of $K_1$ and $K_2$ in a <strong>2-Head Attention</strong> would each be $512 \times 256$, thus projecting to a 256-dimensional space instead of 512-dimensional.</p><p>After all the heads compute its own value of <code>Attention(X,Q_i,K_i,V_i)</code> in parallel, we concatenate the outputs to obtain an output of the same shape as we had in the case of single-head attention. This is followed by a final linear projection to $d_{emb}$-dimensional space where $d_{emb}$ is the embedding dimension. For our example input of shape <code>(9,512)</code>, MHA produces an output of the same shape.</p><p>The intuition behind why having multiple heads improves performance is that by having independently trainable linear projections per head, the model is able to simultaneously attend to different aspects of the language (for example, for a model trained on LaTeX documents, it might have one head that learns to attend to a presence of a <code>\end</code> command if a <code>\begin</code> command appears in the sequence, and another head that relates words in terms of their semantic relevance in text).</p><h2 id=back-to-positional-encodings>Back to Positional Encodings</h2><p>Now we can finally talk about why we need positional encodings. We&rsquo;ve seen that (self-)attention basically comes down to taking a bunch of dot products and outputting a new vector with this information. The problem is, by simply taking dot products, we lose information about the relative order of these words in a sentence. And we know that the position of a word in a sentence matters.</p><p>To encode information about the position of each token in the sequence, we add <strong>positional encodings</strong> to the input embeddings. In practice, there are many ways to generate this &ndash; including having the network learn this during training &ndash; but the authors use the following formula:</p><p>$$\begin{aligned}
PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{emb}}) \
PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{emb}})
\end{aligned}$$</p><p>where $i$ is the index along the embedding dimension and $pos$ is the position of the token in the sequence. Both are 0-indexed. By having sine and cosine functions of varying periods, we are able to inject information about position in continuous form.</p><p align=center style=display:flex;flex-direction:column;align-items:center><img src=https://sarckk.github.io/media/download.png width=400><caption>Illustration of positional encodings</caption></p><p>I don&rsquo;t have much to add on positional encodings, though I&rsquo;ll point out that the periodic nature of sinusoids used here has some nice properties, like placing more emphasis on <strong>relative</strong> &ndash; as opposed to absolute &ndash; order.</p><details><summary>Implementation of Positional Encoding layer</summary>
There are many ways to implement this, but I've chosen to do it this way:<pre><code class=language-python>class PositionalEncoding(nn.Module):
  def __init__(self, emb_dim: int, max_seq_len: int = 5000):
    super().__init__()
    assert emb_dim % 2 == 0, &quot;Embedding dimension must be divisble by 2&quot;
    self.dropout = nn.Dropout(0.1)
    
    pos = torch.arange(max_seq_len)[:, None] # [seq_len, 1]
    evens = 10000. ** (-torch.arange(0,emb_dim,step=2) / emb_dim)
    evens = evens[None, :] # [1, ceil(emb_dim/2)]
    evens = pos * evens # [seq_len, ceil(emb_dim/2)]
    pe = rearrange([evens.sin(), evens.clone().cos()], 't h w -&gt; h (w t)') # interleave even and odd parts
      
    self.register_buffer('pe', pe) # [max_seq_len, emb_dim]
  
  def forward(self, 
              src: Tensor # [bsz, seq_len, emb_dim]
              ) -&gt; Tensor:
    assert src.shape[-1] == self.pe.shape[1], f&quot;Expected embedding dimension of {self.pe[1]} but got {src.shape[-1]} instead.&quot;
    out = src + self.pe[None,:src.size(1),:]
    return self.dropout(out) # See Page 7 of original paper, under section &quot;Regularization&quot;
</code></pre><p>Note that the <code>self.register_buffer('pe', pe)</code> line is important because while the positional encodings do not have trainable parameters, this adds the encoding to the model&rsquo;s parameters and ensures that it is saved during <code>torch.save()</code>.</p></details><h2 id=masking>Masking</h2><h1 id=feed-forward-networks>Feed Forward Networks</h1><p>After</p><h1 id=residual-connections--layer-normalization>Residual Connections + Layer Normalization</h1><p>TODO</p><h1 id=decoder>Decoder</h1><p>Phew! There was quite a lot to cover for Encoders. Fortunately, I&rsquo;ve already covered most of the important parts of the Transformer, and the decoding part more or less mimics what we had in the encoding phase.</p><p>Similar to encoding, we have $N$ Decoder modules &ndash; all independently trained. Each Decoder is similar to the Encoder, except we have an additional stage of Attention computation + feed forward network. The difference is that this attention is called <strong>cross-attention</strong>, where we use the final encoder output (remember this is a tensor of shape <code>(B,T,D)</code>) as the key and value tensors for attention, while the queries come from the decoder itself (input for the first decoder, and the output of the previous decoder for subsequent decoders).</p><h2 id=cross-attention>Cross-Attention</h2><p>Cross-attention is simlar, except we use (TODO)</p><h2 id=masking-1>Masking</h2><p>Masking is slightly different for Decoders, too. Instead of&mldr;</p><h1 id=linear--softmax-layer>Linear + Softmax Layer</h1><h1 id=conclusion>Conclusion</h1><p>TODO</p><p>The code for the article can be found on my <a href=https://github.com/sarckk/transformer_series/blob/main/transformers.ipynb target=_blank rel="noreferrer noopener">github</a>
. My aim was to get the training working as quickly as possible so it&rsquo;s far from polished.</p></main><footer><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
</script><script defer src=//yihui.org/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//yihui.org/js/center-img.js></script><hr><a href=https://github.com/sarckk target=_blank rel="noreferrer noopener">Github</a></footer></body></html>